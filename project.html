<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">


<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Menglong Zhu's homepage - Research</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<link rel="stylesheet" type="text/css" href="css.css" media="screen" />
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-49580934-1', 'upenn.edu');
    ga('require', 'displayfeatures');
    ga('send', 'pageview');
</script>

</head>

<body>

<div id="container"> 

<div id="header">
<div class="headtitle">Menglong Zhu</div>
<div class="headerpic"><img src="profile.jpg" /></div>
</div>

<div id="menu"> 
<ul>
<li><a href="index.html" title="">HOME</a></li>
<!--<li><a href="" title="">ABOUT ME</a></li>-->
<li><a href="project.html" class="selected" title="">RESEARCH</a></li>
<!--<li><a href="teaching.html" title="">TEACHING</a></li>-->

</ul>
</div>


<div id="content"> 
    <div id="insidecontent"> <br />
        <h1>Object Detection and Pose Estimation</h1>
        <div class="content">
            <strong>Single Image 3D Object Detection and Pose Estimation 
                for Grasping, </strong> <br />
            <strong>M. Zhu</strong>, K. Derpanis, Y. Yang, S. Brahmbhatt, 
            M. Zhang, C. Phillips, M. Lecce and K. Daniilidis,  
            International Conference on Robotics and Automation 
            (<strong>ICRA</strong>), 2014.
            [<a href="papers/icra2014_object_grasping.pdf" 
                target="_blank">pdf</a>]
        </div>

        <div class="content">
            In this paper, we address the problem of a robot grasping
            3D objects of known 3D shape from their projections in
            single images of cluttered scenes. 
            We present a novel approach for detecting objects
            and estimating their 3D pose in single images of cluttered
            scenes. 
        </div>

        <div class="content">
            <iframe width="560" height="355" 
                src="http://www.youtube.com/embed/lLKxeaj6EMo?vq=hd720" 
                frameborder="0" allowfullscreen></iframe>
        </div>
    </div>

    <div id="insidecontent">
        <h1>Action Recognition and Detection</h1>

        <div class="content">
            <strong>From Actemes to Action: A Strongly-supervised Representation
                for Detailed Action Understanding,</strong>  
            W. Zhang, <strong>M. Zhu</strong> and K. Derpanis, 
            International Conference on Computer Vision 
            (<strong>ICCV</strong>), 2013.
            [<a href="papers/iccv13_acteme.pdf" target="_blank">pdf</a>] 
        </div>

        <div class="content">
            Human action classiﬁcation (“What action is present in
            the video?”) and detection (“Where and when is a particular
            action performed in the video?”) are key tasks 
            for understanding imagery.
            This paper presents a novel approach for analyzing
            human actions in non-scripted, unconstrained video 
            settings based on volumetric, x-y-t, patch classiﬁers, 
            termed actemes.
        </div>
        
        <div class="content">
            <iframe width="560" height="355" 
                src="http://www.youtube.com/embed/RjlFNGINFwE" 
                frameborder="0" allowfullscreen></iframe>
        </div>
    </div>


    <div id="insidecontent">
        <h1>Monocular 3D</h1>
        <div class="content">
            <strong>Monocular Visual Odometry and Dense 3D Reconstruction 
              for On-Road Vehicles,</strong> <br />
          <strong>M. Zhu</strong>, S. Ramalingam, Y. Taguchi and T. Garass, 
          European Conference on Computer Vision, 
          workshop on Computer Vision in Vichecle Technology 
          (<strong>ECCV</strong>), 2012.
          [<a href="papers/mono-visual-odom-eccv2012-workshop.pdf" 
              target="_blank">pdf</a>]
        </div>

        <div class="content">
            More and more on-road vehicles are equipped with cameras
            each day. Accurate ego-motion estimation of a vehicle from 
            video sequences is a challenging and important problem in 
            robotics and computer vision. This paper presents a novel 
            method for estimating the relative motion of a vehicle 
            from a sequence of images obtained using a single 
            vehicle-mounted camera.
        </div>
        <div class="content">
            <iframe width="560" height="355" 
                src="http://www.youtube.com/embed/2Ym9N0ZGGB4" 
                frameborder="0" allowfullscreen></iframe>
        </div>
    </div>

    <div id="insidecontent"> <br>
        <h1>Literate PR2</h1>

        <div class="content">
            Reading texts from natural scene is an easy task for human being
            but not for a robot. However our PR2 robot, Graspy, became quite smart 
            in this case. In this project, Graspy is programed to be able 
            to detect and recognize the text from an indoor environment. 
            What's more, Graspy is so excited to have this ability that he
            reads out every single word he sees!
        </div>

        <div class="content">
            literate_pr2 is now an open source ROS package: 
            <a href="http://www.ros.org/wiki/literate_pr2">
                http://www.ros.org/wiki/literate_pr2</a>.
            To learn more about ROS and PR2, take a look at 
            <a href="http://www.willowgarage.com/">
                Willow Garage's website</a>
        </div>

        <div class="content">
            <iframe width="560" height="315" 
                src="http://www.youtube.com/embed/AYeApDjsmpQ" 
                frameborder="0" allowfullscreen></iframe>
        </div>

        <br>
        <br>
        </p>
        <p> In the Press </p>
        <p> <a href='http://singularityhub.com/2011/05/18/storybook-time-with-the-pr2-this-robot-can-read-anything-it-sees/'>Singularity Hub</a>  </p>
        <p> <a href='http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/pr2-learns-to-read-cant-pronounce-robot'>ieee spectrum</a> </p>
        <p> <a href='http://news.cnet.com/pr2-robot-learns-to-read-follows-words-anywhere/8301-17938_105-20065448-1.html'>cnet</a> </p>
        <p> <a href='http://www.upenn.edu/pennnews/current/2011-06-09/latest-news/grasp-lab-teaches-robot-read'>Penn News</a> </p>
        <p> <a href='http://www.popsci.com/technology/article/2011-05/video-can-we-please-have-our-own-pr2-thanks'>Popsci</a> </p>
        <p> <a href='http://www.ubergizmo.com/2011/05/graspy-pr2-robot-read/'>ubergizmo</a> </p>
        <p>......</p>
        <br /><br /><br />
    </div>

    <div style="clear: both;"></div>
  </div>

  <div id="footer"> 
  </div>
  <!-- <span>DreamTemplate | Copyright © 2005 | All Rights Reserved  </span -->
</div>
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=7399665; 
var sc_invisible=1; 
var sc_security="3f2f776a"; 
</script>
<script type="text/javascript"
src="http://www.statcounter.com/counter/counter.js"></script>
<noscript><div class="statcounter"><a title="tumblr hit
tracking tool" href="http://statcounter.com/tumblr/"
target="_blank"><img class="statcounter"
src="http://c.statcounter.com/7399665/0/3f2f776a/0/"
alt="tumblr hit tracking tool"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
</body>
</html>
